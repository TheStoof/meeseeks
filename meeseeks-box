#!/usr/bin/env python3

import sys, os, time
import logging
import threading, subprocess, select
import uuid
import json
import random
import base64
import socket, socketserver, ssl


class RequestHandler(socketserver.StreamRequestHandler):
    '''control socket request handler'''
    def handle(self):
        self.logger=logging.getLogger(str(self.client_address))
        self.logger.debug('connected')
        while True:
            l=self.rfile.readline() #get line from client
            if not l: break #will be None if client disconnected
            requests=json.loads(l) #apply initial config
            responses=[]
            if requests:
                for request in requests:
                    response=self.server.handler.handle(request)
                    responses.append(response)
            self.wfile.write(json.dumps(responses).encode())
            self.wfile.write('\n'.encode())
            self.wfile.flush() #flush
        self.logger.debug('disconnected')

class RequestListener (socketserver.ThreadingMixIn, socketserver.TCPServer):
    '''control socket'''
    allow_reuse_address=True
    def get_request(self):
        newsocket, fromaddr = self.socket.accept()
        sslcfg=self.cfg.get('ssl')
        if sslcfg:
            connstream = ssl.wrap_socket(newsocket,
                        server_side=True,
                        certfile = sslcfg.get('certfile'),
                        keyfile = sslcfg.get('keyfile'),
                        ca_certs = sslcfg.get('ca_certs') )
            return connstream, fromaddr
        return newsocket, fromaddr

    
class State(threading.Thread):
    '''cluster state handler'''
    def __init__(self,node,refresh=10,expire=300):
        self.node=node
        threading.Thread.__init__(self,daemon=True,name=self.node+'.State',target=self.state_run)
        self.logger=logging.getLogger(self.name)

        self.shutdown=threading.Event()

        self.refresh=refresh
        self.expire=expire
    
        '''state is:
            __jobs: { jid: {pool: node: ts: state: [jobargs...] }} }
            jid: uuid of the job
            ts: the last updated timestamp of the job
            pool: the pool (queue) the job runs in 
            node: the node the job is assigned to
            state: the job state
        '''
        self.__lock=threading.Lock() #lock on __jobs dict
        self.__jobs={} #(partial) cluster job state, this is private because we lock during any changes
        self.pool_status={} #map of [pool][node][open slots] for nodes we connect downstream to
        self.node_status={} #map of node:last status

        self.start()

    def set_node_status(self,node,**kwargs): self.node_status.setdefault(node,{}).update(**kwargs)
    def set_pool_status(self,pool,node,slots): self.pool_status.setdefault(pool,{})[node]=slots

    def get(self,node=None,pool=None,ts=None):
        #dump all state for a node/pool/or updated after a certain ts
        with self.__lock:
            try: 
                return dict((jid,job.copy()) for (jid,job) in self.__jobs.items() if \
                    (   (not node or job['node']==node) and \
                        (not pool or job['pool']==pool) and \
                        (not ts or job['ts']>ts) )
                )
            except Exception as e: self.logger.warning(e,exc_info=True)
        return None

    def sync(self,jobs={},status={}):
        #update local state with incoming state if job not in local state or job ts >= local jobs ts
        with self.__lock:
            updated=[]
            try:
                for jid,job in jobs.items():
                    if jid not in self.__jobs or self.__jobs[jid]['ts'] <= job['ts']: 
                        self.__jobs.setdefault(jid,{}).update(job)
                        updated.append(jid)
            except Exception as e: self.logger.warning(e,exc_info=True)
        #update node status and pool availabiliy
        try:
            for node,node_status in status.get('nodes',{}).items():
                self.set_node_status(node,**node_status)
            for pool,nodes in status.get('pools',{}).items():
                for node,slots in nodes.items(): self.set_pool_status(pool,node,slots)
        except Exception as e: self.logger.warning(e,exc_info=True)
        #return updated items
        return updated

    def get_job(self,jid):
        #get job by ID
        with self.__lock:
            try:
                if jid in self.__jobs: return self.__jobs.get(jid).copy()
            except Exception as e: self.logger.warning(e,exc_info=True)
    
    def update_job(self,jid,**data):
        #get job by ID
        with self.__lock:
            try:
                if jid in self.__jobs: 
                    self.__jobs[jid].update(ts=time.time(),**data)
                    return self.__jobs.get(jid)
                else: return False
            except Exception as e: self.logger.warning(e,exc_info=True)

    def add_job(self,**jobargs):
        #add a new job to the state
        with self.__lock:
            try:
                if 'pool' not in jobargs or 'cmd' not in jobargs: return False #jobs have to have a command and pool to run in
                jid=jobargs.get('id',str(uuid.uuid1())) #use preset id or generate one
                if jid in self.__jobs: #job exists
                    self.logger.warning('add_job: %s exists'%jid)
                    return False
                self.__jobs[jid]={  'ts':time.time(),           #last updated timestamp
                                    'submit_ts':time.time(),    #submit timestamp
                                    'start_ts':None,            #job start
                                    'end_ts':None,              #job end
                                    'pool':None,                #pool to run in 
                                    'nodelist':[],              #list of nodes allowed to handle this job
                                    'node':self.node,           #node job is currently on
                                    'state':'new'               #job state
                                }             
                self.__jobs[jid].update(jobargs)
                return jid
            except Exception as e: self.logger.warning(e,exc_info=True)

    def state_run(self):
        self.logger.info('started')
        while not self.shutdown.is_set():
            self.logger.debug(self.node_status)
            self.logger.debug(self.pool_status)
            #look for jobs that should have been updated
            with self.__lock:
                try:
                    for jid,job in self.__jobs.copy().items():
                        if time.time()-job['ts'] > self.expire: 
                            #jobs that have ended for some reason will no longer be updated, so expire them.
                            if job['state'] in ['done','killed','failed']:
                                self.logger.debug('expiring job %s'%jid)
                                del self.__jobs[jid]
                            else: 
                                #this job *should* have been updated
                                self.logger.warning('job %s not updated in %s seconds'%(jid,self.expire))
                                #if we restart on fail and have a nodelist
                                if job.get('restart_on_fail') and job['nodelist']:
                                    # try kicking it back to the first node for rescheduling
                                    self.__jobs[jid].update(node=job['nodelist'][0]) 
                                #set job to failed, it might restart if it can
                                self.__jobs[jid].update(ts=time.time(),state='failed',error='expired')
                                
                                
                except Exception as e: self.logger.warning(e,exc_info=True)
            #set nodes that have not sent status to offline
            for node,node_status in self.node_status.items():
                if node_status.get('online') and time.time()-node_status['ts'] > self.expire:
                    self.logger.warning('node %s not updated in %s seconds'%(node,self.expire))
                    node_status.update(online=False)
            time.sleep(self.refresh)

class Node(threading.Thread):
    '''node poller/state sync thread
    initially we try to push all state to the node (sync_ts of 0)'''
    def __init__(self,node,remote_node,state,refresh=10,address=None,port=13700,timeout=10,**cfg):
        self.node=node #node we are running on 
        self.remote_node=remote_node #node we connect to
        self.state=state
        threading.Thread.__init__(self,daemon=True,name='Node.'+self.remote_node,target=self.node_run)
        self.logger=logging.getLogger(self.name)
        self.address=address
        if not self.address: self.address=self.node
        self.port=port
        self.timeout=timeout
        self.refresh=refresh
        self.__socket=None
        self.cfg=cfg
        self.shutdown=threading.Event()
        self.start()

    def __sr(self,requests):
        #conected and send/recieve request/response
        if not self.__socket:
            self.logger.debug('connecting to %s:%s'%(self.address,self.port))
            try: 
                self.__socket=socket.create_connection((self.address,self.port),timeout=self.timeout)
                sslcfg=self.cfg.get('ssl')
                if sslcfg:
                    self.__socket = ssl.wrap_socket(self.__socket,
                        certfile = sslcfg.get('certfile'),
                        keyfile = sslcfg.get('keyfile'),
                        ca_certs = sslcfg.get('ca_certs') )
            except Exception as e:
                if self.__socket is not False:
                    self.logger.warning("%s:%s %s"%(self.address,self.port,e))
                    self.__socket=False #suppress repeated warnings
            if self.__socket: self.logger.info('connected to %s:%s'%(self.address,self.port))
        if self.__socket:
            try:
                self.__socket.sendall(json.dumps(requests).encode())
                self.__socket.sendall('\n'.encode())
                l=''
                while True:
                    l+=self.__socket.recv(65535).decode()
                    if '\n' in l: return json.loads(l)
            except Exception as e: 
                self.logger.warning(e,exc_info=True)
                if self.__socket: 
                    self.__socket.close()
                    self.__socket=None

    def node_run(self):
        while not self.shutdown.is_set():
            if not self.__socket: ts=0 #reset ts to push all state on reconnect

            #we need to send updates that are for:
            # the node we are connecting to
            # nodes we know about but do not directly connect to
            peers=self.state.node_status.get(self.node,{}).get('peers',[])
            sync=dict( (jid,job) for (jid,job) in self.state.get(ts=ts).items() if \
                    job['node'] != self.node and \
                        ( job['node'] == self.remote_node or job['node'] not in peers ) 
                )
            #create the request
            request={
                'status':{},
                #dump all jobs for this node updated more recently than the last sync
                'sync':sync,
                'get':{'ts':ts}
            }
            responses=self.__sr([request])
            updated=None
            if responses:
                response=responses[0]
                updated=self.state.sync(
                    response.get('get',{}),
                    response.get('status',{})
                )
            if responses: self.logger.debug('%s sent %s, updated %s'%(ts,len(sync),len(updated)))
            ts=time.time()-self.refresh #set the next window to go back to one refresh period ago
            time.sleep(self.refresh) 
        if self.__socket:self.__socket.close()


class Task(threading.Thread):
    '''subprocess manager'''        
    def __init__(self,job):
        threading.Thread.__init__(self)
        popen_args={}
        self.stdin=self.stdout=self.stderr=None #file handles if redirecting
        #stdin from file
        stdin=job.get('stdin')
        if stdin:
            self.stdin=open(stdin,'rb')
            popen_args.update(stdin=self.stdin)
        #stdout to file, or to pipe if True
        stdout=job.get('stdout')
        if stdout is True: popen_args.update(stdout=subprocess.PIPE)
        elif stdout: 
            self.stdout=open(stdout,'ab')
            popen_args.update(stdout=self.stdout)
        #stderr to file, or pipe
        stderr=job.get('stderr')
        if stderr is True: popen_args.update(stderr=subprocess.PIPE)
        elif stderr: 
            self.stderr=open(stderr,'ab')
            popen_args.update(stderr=self.stderr)
        #start subprocess
        self.__sub=subprocess.Popen(job.get('cmd'),**popen_args)
        self.pid=self.__sub.pid
        self.start() #thread will wait on subprocess
    
    def run(self): 
        #block here until process finishes
        stdout,stderr=self.__sub.communicate()
        #close file handles
        if self.stdin: self.stdin.close()
        if self.stdout: self.stdout.close()
        if self.stderr: self.stderr.close()
        #return output as a base64 string if we got any
        if stdout: self.stdout=base64.b64encode(stdout).decode()
        else: self.stdout=None
        if stderr: self.stderr=base64.b64encode(stderr).decode()
        else: self.stderr=None

    def kill(self): self.__sub.kill()
    def poll(self): return self.__sub.poll()

class Pool(threading.Thread):
    '''job queue manager'''
    def __init__(self,node,pool,state,refresh=10,update=60,slots=None,max_runtime=None):
        self.node=node #node we are running on
        self.pool=pool #pool we service
        self.state=state #state thread
        self.refresh=refresh #how often we update the job q
        self.update=update #how often we update the state of running/waiting jobs
        self.max_runtime=max_runtime
        self.slots=slots #number of job slots, or None if not limited
        threading.Thread.__init__(self,daemon=True,name='Pool.'+self.pool,target=self.pool_run)
        self.logger=logging.getLogger(self.name)
        self.shutdown=threading.Event()
        self.__tasks={} #map of job_id -> Task object
        self.start()

    def slots_free(self):
        #returns how many slots are available or True if not set
        if not self.slots: return None
        else: return self.slots-len(self.__tasks)

    def start_job(self,jid):
        job=self.state.get_job(jid)
        try: 
            self.__tasks[jid]=Task(job)
            pid=self.__tasks[jid].pid
            self.state.update_job(jid,
                state='running',
                start_ts=time.time(),
                pid=pid,
            )
            self.logger.info('started %s [%s]'%(jid,pid))
        except Exception as e:
            self.logger.warning(e,exc_info=True)
            self.state.update_job(jid,state='failed',error=str(e))
    
    def kill_job(self,jid,job):
        self.logger.info('killing %s [%s]'%(jid,job['pid']))
        self.__tasks[jid].kill()
        self.state.update_job(jid,state='killed')

    def check_job(self,jid,job):
        state=job['state']
        rc=self.__tasks[jid].poll()
        if rc is not None: #job exited
            self.__tasks[jid].join() #wait for task to exit
            if state == 'running':
                if rc == 0: state='done'
                else: state='failed'
            self.state.update_job( jid,
                state=state,
                end_ts=time.time(),
                rc=rc, pid=None,
                stdout=self.__tasks[jid].stdout,
                stderr=self.__tasks[jid].stderr )
            self.logger.info("job %s %s (%s)"%(jid,state,rc))
            del self.__tasks[jid] #and dispose of it
        #was job killed or max runtime
        elif job.get('max_runtime') and (time.time()-job['start_ts'] > job['max_runtime']):
            self.logger.warning('job %s exceeded job max_runtime of %s'%(jid,job['max_runtime']))
            self.kill_job(jid,job)
        elif self.max_runtime and (time.time()-job['start_ts'] > self.max_runtime):
            self.logger.warning('job %s exceeded pool max_runtime of %s'%(jid,self.max_runtime))
            self.kill_job(jid,job)

    def pool_run(self):
        while not self.shutdown.is_set():
            try:
                #get jobs assigned to this node and pool
                pool_jobs=self.state.get(node=self.node,pool=self.pool)
                for jid,job in pool_jobs.items():
                    #check running jobs
                    if jid in self.__tasks:
                        if job['state'] == 'killed': self.kill_job(jid,job)
                        self.check_job(jid,job)
                    #update queued/running jobs
                    if (job['state'] == 'running' or job['state'] == 'waiting') \
                        and (time.time()-job['ts'] > self.update):
                            self.state.update_job(jid) #touch it so it doesn't expire
                    #check to see if we can start a job
                    elif job['state'] == 'new' or job['state'] == 'waiting' \
                        or (job['state'] == 'done' and job.get('restart_on_done')) \
                        or (job['state'] == 'failed' and job.get('restart_on_fail')):
                            if (not self.slots) or (len(self.__tasks) < self.slots): self.start_job(jid)
                            #job is waiting for a slot
                            elif job['state'] != 'waiting': self.state.update_job(jid,state='waiting')
            except Exception as e: self.logger.error(e,exc_info=True)
            self.state.set_pool_status(self.pool,self.node,self.slots_free())
            time.sleep(self.refresh)

        #at shutdown, kill all jobs
        for jid,job in self.__tasks.copy().items(): self.kill_job(jid,job)

class Main:
    '''meeseeks box main thread'''
    def __init__(self,nodes={},pools={},**cfg):
        #load config defaults
        self.defaults=cfg.get('defaults',{})
        
        #get our nodename
        self.name=cfg.get('name',socket.gethostname())

        #set up logger
        self.logger=logging.getLogger(self.name+'.main')

        #init state
        scfg=self.defaults.copy()
        scfg.update(cfg.get('state',{}))
        self.state=State(self.name,**scfg)

        #start listening
        listencfg=self.defaults.copy()
        listencfg.update(cfg.get('listen',{})) #merge in listener specifc options
        self.listener=RequestListener( (  listencfg.get('address','localhost'),
                                            listencfg.get('port',13700)   ), 
                            RequestHandler)
        self.listener.cfg=listencfg #for passing ssl params and other options
        self.listener.handler=self
        self.listener.server_thread=threading.Thread(target=self.listener.serve_forever)
        self.listener.server_thread.daemon=True
        self.listener.server_thread.start()
        self.logger.info('listening on %s:%s'%self.listener.server_address)

        #init pools
        self.pools={} #pools we process jobs for
        for p in pools.keys():
            pcfg=self.defaults.copy()
            pcfg.update(pools[p])
            self.logger.info('creating pool %s'%p)
            self.pools[p]=Pool(self.name,p,self.state,**pcfg)

        #init node threads
        self.nodes={} #nodes we sync
        for n in nodes.keys():
            ncfg=self.defaults.copy()
            ncfg.update(nodes[n])
            self.logger.info('adding node %s'%n)
            self.nodes[n]=Node(self.name,n,self.state,**ncfg)

    def get_loadavg(self):
            with open('/proc/loadavg') as fh:
                try: return float(fh.readline().split()[0])
                except Exception as e: self.logger.warning(e,exc_info=True)

    def biased_random(self,l,reverse=False):
        #this is probably a stupid way to pick a random item while favoring the lowest sorted items 
        #first we pick a range between the first item and a randomly selected item
        #them we pick a random item from that range
        if l: 
            l=sorted(l,reverse=reverse)
            if len(l) > 1: return random.choice( l[ 0:random.randint(1,len(l)) ] )
            else: return l[0] #only one item...

    def select_by_loadavg(self,nodes):
        #loadvg,node sorted low to high
        loadavg_nodes=[ (self.state.node_status[node].get('loadavg'), node) for node in nodes \
            if self.state.node_status[node].get('loadavg') is not None ] 
        #do we have any valid load averages?
        if loadavg_nodes: return self.biased_random(loadavg_nodes)[1]
        #if we have no valid load averages, pick a random node from the pool
        else: return random.choice( nodes )
    
    def select_by_available(self,pool,nodes):
        #get nodes in this pool sorted from most to least open slots
        nodes_slots=[ (self.state.pool_status[pool][node], node) for node in nodes \
            if self.state.pool_status[pool][node] is not None ]
        #do we have any nodes with pool slots?
        if nodes_slots: 
            s,node=self.biased_random(nodes_slots,reverse=True)
            if s > 0: self.state.set_pool_status(pool,node,s-1) #update the local free slot count
            return node
        #if not, pick a random node from the pool
        else: return random.choice( nodes )

    def run(self):
        self.logger.info('running')
 
        while True: #existence is pain!
            #these are the nodes we talk to
            peers=list(self.nodes.keys()) 
            #update our node status
            self.state.set_node_status( self.name,
                online=True,
                ts=time.time(),
                loadavg=self.get_loadavg(),
                peers=peers )

            #update our pool status for pools we don't have to show how many slots are free downstream
            for pool in self.state.pool_status.keys():
                if pool in self.pools: continue #we have this pool locally
                slots=0
                for peer in peers: #total up free slots downstream
                    s=self.state.pool_status[pool].get(peer,None)
                    if s: slots+=s
                self.state.set_pool_status(pool,self.name,slots)
    
            #job routing logic
            try:
                #get jobs assigned to us
                for jid,job in self.state.get(node=self.name).items():
                    pool=job['pool']
                    #if we can service this job, the pool thread will claim the job so do nothing
                    if pool not in self.pools: 
                        try:
                            #we need to forward to an online node that has the job's pool
                            nodes=[ node for node in self.state.pool_status.get(pool,{}).keys() \
                                if node in peers and self.state.node_status[node].get('online') ]
                            if not nodes: continue #we can't do anything with this job yet

                            #filter by the job's nodelist if set
                            if job['nodelist']: 
                                in_list_nodes=[node for node in nodes if node in job['nodelist']]
                                #if we got a result, use it
                                #we may not if the nodelist only controlled the upstream routing
                                #so if we got nothing based on the node list use all pool nodes
                                if in_list_nodes: nodes=in_list_nodes

                            #select a node the job
                            if self.defaults.get('use_loadavg'): node=self.select_by_loadavg(nodes)
                            else: node=self.select_by_available(pool,nodes)

                            #route the job
                            self.logger.debug('routing %s for %s to %s'%(jid,pool,node))
                            self.state.update_job(jid,node=node)
                            
                        except Exception as e: self.logger.warning(e,exc_info=True)
                
                time.sleep(1)

            except KeyboardInterrupt: break
            except Exception as e: self.logger.error(e,exc_info=True)

        self.logger.info('shutting down')
        try:
            self.listener.shutdown()
            self.listener.server_thread.join()
        except Exception as e: self.logger.error(e,exc_info=True)

        #stop all pools
        for pool in self.pools.values():
            pool.shutdown.set()
            pool.join()

        #stop all nodes
        for node in self.nodes.values():
            node.shutdown.set()
            node.join()

        #stop state manager
        self.state.shutdown.set()
        self.state.join()    

    #handle incoming request
    def handle(self,request):
        response={}
        #we're being pushed state from upstream node and should return ours
        if 'sync' in request:
            #sync incoming state, return updated job ids
            response['sync']=self.state.sync(request['sync'])
        #return our state
        if 'get' in request:
            response['get']=self.state.get(**request['get'])
        #submit job
        if 'submit' in request: 
            response['submit']=self.state.add_job(**request['submit'])
        #query job
        if 'query' in request: response['query']=self.state.get_job(request['query'])
        #modify job
        if 'modify' in request:
            for jid,data in request['modify'].items():
                response.setdefault('modify',{})[jid]=self.state.update_job(jid,**data)
        #kill job
        if 'kill' in request:
                response['kill']=self.state.update_job(request['kill'],state='killed')
        #get cluster status
        if 'status' in request: 
            response['status']={     
                #return the node status
                'nodes':self.state.node_status,
                #return the pool status
                'pools':self.state.pool_status
            }
        return response


if __name__ == '__main__':
    cfg={} 
    for f in sys.argv[1:]: 
        try:
            with open(f) as fh: cfg.update(json.load(fh))
        except Exception as e: print (e,file=sys.stderr)
    logging.basicConfig(**cfg.get('logging',{'level':logging.INFO}))
    Main(**cfg).run()
